{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "i1cdzb7zI13H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5bfbc9d-54a1-4132-c727-31004f45c7f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-678d8de59d4e>:42: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "  df2['userid'] = pd.Series([])\n",
            "<ipython-input-13-678d8de59d4e>:43: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "  df2['Maximum Content Similarity'] = pd.Series([])\n",
            "<ipython-input-13-678d8de59d4e>:104: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "  df2['userid'] = pd.Series([])\n",
            "<ipython-input-13-678d8de59d4e>:105: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "  df2['Maximum Content Similarity'] = pd.Series([])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32,\n",
              " 211,\n",
              " 11    true to size and fits like a glove no need to ...\n",
              " 12    i bought these for my son and he absolutely lo...\n",
              " 13    the retro vibe of these jordans is nostalgic a...\n",
              " 14    they look even better in person than in the pi...\n",
              " 15    you can t beat the timeless style of jordan sn...\n",
              " 16    i ve never received so many compliments on my ...\n",
              " 17    versatile enough to pair with any outfit casua...\n",
              " 18    the grip on these shoes is fantastic making th...\n",
              " 19    the packaging and unboxing experience are as p...\n",
              " 20    attention sneakerheads these are a must cop fo...\n",
              " 21    the leather is so soft it feels like a second ...\n",
              " 22    even after hours of wear my feet still feel co...\n",
              " 23    i love the limited edition colorway i snagged ...\n",
              " 24    these jordans add that extra edge to my street...\n",
              " 25    the arch support is on point no more sore feet...\n",
              " 26    these are not just shoes they re a statement p...\n",
              " 27    i appreciate the commitment to sustainability ...\n",
              " 28    my basketball game has improved since i starte...\n",
              " 29    the ankle support is excellent making them gre...\n",
              " 30    i ve had mine for months and they still look b...\n",
              " 31    the retro design pays homage to the origins of...\n",
              " 32    i wore these to a party and everyone wanted to...\n",
              " 33    the jordan brand never disappoints consistentl...\n",
              " 34    a perfect blend of style and functionality can...\n",
              " 35    these shoes feel like a piece of history on my...\n",
              " 36    the packaging is so sleek it s like opening a ...\n",
              " 37    i ve been a fan of jordans for years and these...\n",
              " 38    the innovative technology used in these shoes ...\n",
              " 39    my go to choice for both athletic activities a...\n",
              " 40    i ve been stopped on the street multiple times...\n",
              " 41    the brand s commitment to social causes makes ...\n",
              " 42    these jordans are a conversation starter where...\n",
              " Name: comment, dtype: object)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "def pretrain(filename):\n",
        "\n",
        "  import pandas as pd\n",
        "  import numpy as np\n",
        "  import matplotlib.pyplot as plt\n",
        "  %matplotlib inline\n",
        "  import seaborn as sns\n",
        "  from nltk.tokenize import RegexpTokenizer\n",
        "  from collections import OrderedDict\n",
        "\n",
        "  import nltk\n",
        "  from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "  from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
        "  from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score, precision_score, f1_score, pairwise_distances\n",
        "  from sklearn.model_selection import train_test_split\n",
        "\n",
        "  df = pd.read_csv(\"false_data.csv\") # dataset\n",
        "\n",
        "  df['comment'] = df.apply(lambda row: str(row['comment']).lower(), axis=1) # lowercase\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  df['comment'] = df['comment'].apply(lambda x: ' '.join(word for word in tokenizer.tokenize(x))) # remove punctuation but word tokenization\n",
        "  df['review_length'] = df['comment'].apply(lambda x: len(x.split())) # review word count\n",
        "  df['date'] = pd.to_datetime(df['ctime'],unit='s').dt.date #date column\n",
        "  df['time'] = pd.to_datetime(df['ctime'],unit='s').dt.time # time column\n",
        "  mnr_df1 = df[['userid', 'date']].copy()\n",
        "  mnr_df2 = mnr_df1.groupby(by=['date', 'userid']).size().reset_index(name='mnr')\n",
        "  mnr_df2['mnr'] = mnr_df2['mnr'] / mnr_df2['mnr'].max() #finds the number of reviews made in 1 day/number of reviews ever made by this user\n",
        "  df = df.merge(mnr_df2, on=['userid', 'date'], how='inner')\n",
        "\n",
        "  review_data = df\n",
        "  res = OrderedDict()\n",
        "\n",
        "  # Iterate over data and create groups of reviewers\n",
        "  for row in review_data.iterrows():\n",
        "      if row[1].userid in res:\n",
        "          res[row[1].userid].append(row[1].comment) #add comment to existing user\n",
        "      else:\n",
        "          res[row[1].userid] = [row[1].comment] # new user\n",
        "\n",
        "  individual_reviewer = [{'userid': k, 'comment': v} for k, v in res.items()]\n",
        "  df2 = dict()\n",
        "  df2['userid'] = pd.Series([])\n",
        "  df2['Maximum Content Similarity'] = pd.Series([])\n",
        "  vector = TfidfVectorizer(min_df=0)\n",
        "  count = -1\n",
        "  for reviewer_data in individual_reviewer:\n",
        "      count = count + 1\n",
        "      try:\n",
        "          tfidf = vector.fit_transform(reviewer_data['comment'])\n",
        "      except:\n",
        "          pass\n",
        "      cosine = 1 - pairwise_distances(tfidf, metric='cosine')\n",
        "\n",
        "      np.fill_diagonal(cosine, -np.inf)\n",
        "      max = cosine.max()\n",
        "\n",
        "      # To handle reviewier with just 1 review\n",
        "      if max == -np.inf:\n",
        "          max = 0\n",
        "      df2['userid'][count] = reviewer_data['userid']\n",
        "      df2['Maximum Content Similarity'][count] = max\n",
        "\n",
        "  df3 = pd.DataFrame(df2, columns=['userid', 'Maximum Content Similarity'])\n",
        "  df = pd.merge(review_data, df3, on=\"userid\", how=\"left\")\n",
        "  df.drop(index=np.where(pd.isnull(df))[0], axis=0, inplace=True) #merge df and df3\n",
        "\n",
        "  df.drop(['itemid', 'userid', 'username', 'ctime', 'rating', 'comment'], axis=1, inplace=True) # remove anything that is not relevant\n",
        "\n",
        "  X = df[['review_length', 'mnr', 'Maximum Content Similarity']]\n",
        "  y = df['fakeornot']\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5, random_state=42) #dataset training\n",
        "\n",
        "  test = pd.read_csv(\"false_data.csv\")\n",
        "  test['comment'] = test.apply(lambda row: str(row['comment']).lower(), axis=1)\n",
        "  # Preprocessing\n",
        "  # Remove Punctuations\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  test['comment'] = test['comment'].apply(lambda x: ' '.join(word for word in tokenizer.tokenize(x)))\n",
        "  # Feature Engineering\n",
        "  test['review_length'] = test['comment'].apply(lambda x: len(x.split()))\n",
        "\n",
        "  # Convert UNIX timestamp to date and time\n",
        "  test['date'] = pd.to_datetime(test['ctime'],unit='s').dt.date\n",
        "  test['time'] = pd.to_datetime(test['ctime'],unit='s').dt.time\n",
        "\n",
        "  # Maximum Number of Reviews per day per reviewer\n",
        "  mnr_df1 = test[['userid', 'date']].copy()\n",
        "  mnr_df2 = mnr_df1.groupby(by=['date', 'userid']).size().reset_index(name='mnr')\n",
        "  mnr_df2['mnr'] = mnr_df2['mnr'] / mnr_df2['mnr'].max()\n",
        "  test = test.merge(mnr_df2, on=['userid', 'date'], how='inner')\n",
        "  # Cosine Similarity\n",
        "  review_data = test\n",
        "  res = OrderedDict()\n",
        "\n",
        "  # Iterate over data and create groups of reviewers\n",
        "  for row in review_data.iterrows():\n",
        "      if row[1].userid in res:\n",
        "          res[row[1].userid].append(row[1].comment)\n",
        "      else:\n",
        "          res[row[1].userid] = [row[1].comment]\n",
        "\n",
        "  individual_reviewer = [{'userid': k, 'comment': v} for k, v in res.items()]\n",
        "  df2 = dict()\n",
        "  df2['userid'] = pd.Series([])\n",
        "  df2['Maximum Content Similarity'] = pd.Series([])\n",
        "  vector = TfidfVectorizer(min_df=0)\n",
        "  count = -1\n",
        "  for reviewer_data in individual_reviewer:\n",
        "      count = count + 1\n",
        "      try:\n",
        "          tfidf = vector.fit_transform(reviewer_data['comment'])\n",
        "      except:\n",
        "          pass\n",
        "      cosine = 1 - pairwise_distances(tfidf, metric='cosine')\n",
        "\n",
        "      np.fill_diagonal(cosine, -np.inf)\n",
        "      max = cosine.max()\n",
        "\n",
        "      # To handle reviewier with just one review\n",
        "      if max == -np.inf:\n",
        "          max = 0\n",
        "      df2['userid'][count] = reviewer_data['userid']\n",
        "      df2['Maximum Content Similarity'][count] = max\n",
        "\n",
        "  df3 = pd.DataFrame(df2, columns=['userid', 'Maximum Content Similarity'])\n",
        "  # left outer join on original datamatrix and cosine dataframe\n",
        "  test = pd.merge(review_data, df3, on=\"userid\", how=\"left\")\n",
        "  df.drop(index=np.where(pd.isnull(df))[0], axis=0, inplace=True)\n",
        "\n",
        "  logreg = LogisticRegression(C=3)\n",
        "  logreg.fit(X_train, y_train)\n",
        "  test['fakeornot'] = 'none'\n",
        "\n",
        "  # Assuming you have already trained a logistic regression model named logreg\n",
        "  # and you have a test set with features 'review_length', 'mnr', 'Maximum Content Similarity'\n",
        "\n",
        "  # Make predictions on the test set\n",
        "  y_pred = logreg.predict(test[['review_length', 'mnr', 'Maximum Content Similarity']])\n",
        "\n",
        "  # Assign the predicted labels to a new column 'fakeornot' in the test set\n",
        "  test['fakeornot'] = y_pred\n",
        "\n",
        "  # Obtain probability estimates for each class\n",
        "  probabilities = logreg.predict_proba(test[['review_length', 'mnr', 'Maximum Content Similarity']])\n",
        "\n",
        "  # Extract the probability of the positive class (class 1)\n",
        "  confidence_level = probabilities[:, 1]\n",
        "\n",
        "  # Add the confidence level to a new column 'confidence_level' in the test set\n",
        "  test['confidence level'] = confidence_level\n",
        "\n",
        "  fake = test.fakeornot.str.count(\"fake\").sum()\n",
        "  original = test.fakeornot.str.count(\"original\").sum()\n",
        "  fake_review = test['comment'].loc[(test.fakeornot == 'fake')]\n",
        "\n",
        "  return (fake,original, fake_review)\n",
        "\n",
        "\n",
        "\n",
        "pretrain('false_data.csv')\n"
      ]
    }
  ]
}