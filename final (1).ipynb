{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "i1cdzb7zI13H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f1b63a3-789b-4be5-d6c5-1ff15a56fb4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-5c67b6c083fb>:47: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "  df2['userid'] = pd.Series([])\n",
            "<ipython-input-7-5c67b6c083fb>:48: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "  df2['Maximum Content Similarity'] = pd.Series([])\n",
            "<ipython-input-7-5c67b6c083fb>:111: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "  df2['userid'] = pd.Series([])\n",
            "<ipython-input-7-5c67b6c083fb>:112: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "  df2['Maximum Content Similarity'] = pd.Series([])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 17, Series([], Name: comment, dtype: object))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "def pretrain(filename):\n",
        "\n",
        "  import pandas as pd\n",
        "  import numpy as np\n",
        "  import matplotlib.pyplot as plt\n",
        "  %matplotlib inline\n",
        "  import seaborn as sns\n",
        "  from nltk.tokenize import RegexpTokenizer\n",
        "  from collections import OrderedDict\n",
        "\n",
        "  from torch import nn\n",
        "  from transformers import Trainer\n",
        "\n",
        "  import joblib\n",
        "\n",
        "  import nltk\n",
        "  from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "  from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
        "  from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score, precision_score, f1_score, pairwise_distances\n",
        "  from sklearn.model_selection import train_test_split\n",
        "\n",
        "  df = pd.read_csv(\"false_data.csv\") # dataset\n",
        "\n",
        "  df['comment'] = df.apply(lambda row: str(row['comment']).lower(), axis=1) # lowercase\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  df['comment'] = df['comment'].apply(lambda x: ' '.join(word for word in tokenizer.tokenize(x))) # remove punctuation but word tokenization\n",
        "  df['review_length'] = df['comment'].apply(lambda x: len(x.split())) # review word count\n",
        "  df['date'] = pd.to_datetime(df['ctime'],unit='s').dt.date #date column\n",
        "  df['time'] = pd.to_datetime(df['ctime'],unit='s').dt.time # time column\n",
        "  mnr_df1 = df[['userid', 'date']].copy()\n",
        "  mnr_df2 = mnr_df1.groupby(by=['date', 'userid']).size().reset_index(name='mnr')\n",
        "  mnr_df2['mnr'] = mnr_df2['mnr'] / mnr_df2['mnr'].max() #finds the number of reviews made in 1 day/number of reviews ever made by this user\n",
        "  df = df.merge(mnr_df2, on=['userid', 'date'], how='inner')\n",
        "\n",
        "  review_data = df\n",
        "  res = OrderedDict()\n",
        "\n",
        "  # Iterate over data and create groups of reviewers\n",
        "  for row in review_data.iterrows():\n",
        "      if row[1].userid in res:\n",
        "          res[row[1].userid].append(row[1].comment) #add comment to existing user\n",
        "      else:\n",
        "          res[row[1].userid] = [row[1].comment] # new user\n",
        "\n",
        "  individual_reviewer = [{'userid': k, 'comment': v} for k, v in res.items()]\n",
        "  df2 = dict()\n",
        "  df2['userid'] = pd.Series([])\n",
        "  df2['Maximum Content Similarity'] = pd.Series([])\n",
        "  vector = TfidfVectorizer(min_df=0)\n",
        "  count = -1\n",
        "  for reviewer_data in individual_reviewer:\n",
        "      count = count + 1\n",
        "      try:\n",
        "          tfidf = vector.fit_transform(reviewer_data['comment'])\n",
        "      except:\n",
        "          pass\n",
        "      cosine = 1 - pairwise_distances(tfidf, metric='cosine')\n",
        "\n",
        "      np.fill_diagonal(cosine, -np.inf)\n",
        "      max = cosine.max()\n",
        "\n",
        "      # To handle reviewier with just 1 review\n",
        "      if max == -np.inf:\n",
        "          max = 0\n",
        "      df2['userid'][count] = reviewer_data['userid']\n",
        "      df2['Maximum Content Similarity'][count] = max\n",
        "\n",
        "  df3 = pd.DataFrame(df2, columns=['userid', 'Maximum Content Similarity'])\n",
        "  df = pd.merge(review_data, df3, on=\"userid\", how=\"left\")\n",
        "  df.drop(index=np.where(pd.isnull(df))[0], axis=0, inplace=True) #merge df and df3\n",
        "\n",
        "  df.head()\n",
        "\n",
        "  df.drop(['userid', 'username', 'ctime', 'rating', 'comment'], axis=1, inplace=True) # remove anything that is not relevant\n",
        "\n",
        "  X = df[['review_length', 'mnr', 'Maximum Content Similarity']]\n",
        "  y = df['fakeornot']\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5, random_state=42) #dataset training\n",
        "\n",
        "  test = pd.read_csv(\"false_data(1).csv\", encoding='ISO-8859-1')\n",
        "  test['comment'] = test.apply(lambda row: str(row['comment']).lower(), axis=1)\n",
        "  # Preprocessing\n",
        "  # Remove Punctuations\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  test['comment'] = test['comment'].apply(lambda x: ' '.join(word for word in tokenizer.tokenize(x)))\n",
        "  # Feature Engineering\n",
        "  test['review_length'] = test['comment'].apply(lambda x: len(x.split()))\n",
        "\n",
        "  # Convert UNIX timestamp to date and time\n",
        "  test['date'] = pd.to_datetime(test['ctime'],unit='s').dt.date\n",
        "  test['time'] = pd.to_datetime(test['ctime'],unit='s').dt.time\n",
        "\n",
        "  # Maximum Number of Reviews per day per reviewer\n",
        "  mnr_df1 = test[['userid', 'date']].copy()\n",
        "  mnr_df2 = mnr_df1.groupby(by=['date', 'userid']).size().reset_index(name='mnr')\n",
        "  mnr_df2['mnr'] = mnr_df2['mnr'] / mnr_df2['mnr'].max()\n",
        "  test = test.merge(mnr_df2, on=['userid', 'date'], how='inner')\n",
        "  # Cosine Similarity\n",
        "  review_data = test\n",
        "  res = OrderedDict()\n",
        "\n",
        "  # Iterate over data and create groups of reviewers\n",
        "  for row in review_data.iterrows():\n",
        "      if row[1].userid in res:\n",
        "          res[row[1].userid].append(row[1].comment)\n",
        "      else:\n",
        "          res[row[1].userid] = [row[1].comment]\n",
        "\n",
        "  individual_reviewer = [{'userid': k, 'comment': v} for k, v in res.items()]\n",
        "  df2 = dict()\n",
        "  df2['userid'] = pd.Series([])\n",
        "  df2['Maximum Content Similarity'] = pd.Series([])\n",
        "  vector = TfidfVectorizer(min_df=0)\n",
        "  count = -1\n",
        "  for reviewer_data in individual_reviewer:\n",
        "      count = count + 1\n",
        "      try:\n",
        "          tfidf = vector.fit_transform(reviewer_data['comment'])\n",
        "      except:\n",
        "          pass\n",
        "      cosine = 1 - pairwise_distances(tfidf, metric='cosine')\n",
        "\n",
        "      np.fill_diagonal(cosine, -np.inf)\n",
        "      max = cosine.max()\n",
        "\n",
        "      # To handle reviewier with just one review\n",
        "      if max == -np.inf:\n",
        "          max = 0\n",
        "      df2['userid'][count] = reviewer_data['userid']\n",
        "      df2['Maximum Content Similarity'][count] = max\n",
        "\n",
        "  df3 = pd.DataFrame(df2, columns=['userid', 'Maximum Content Similarity'])\n",
        "  # left outer join on original datamatrix and cosine dataframe\n",
        "  test = pd.merge(review_data, df3, on=\"userid\", how=\"left\")\n",
        "  df.drop(index=np.where(pd.isnull(df))[0], axis=0, inplace=True)\n",
        "\n",
        "  logreg = LogisticRegression(C=3)\n",
        "  logreg.fit(X_train, y_train)\n",
        "  joblib.dump(logreg, 'pretrained_model.joblib')\n",
        "  logreg = joblib.load('pretrained_model.joblib')\n",
        "  test['fakeornot'] = 'none'\n",
        "\n",
        "  # Assuming you have already trained a logistic regression model named logreg\n",
        "  # and you have a test set with features 'review_length', 'mnr', 'Maximum Content Similarity'\n",
        "\n",
        "  # Make predictions on the test set\n",
        "  y_pred = logreg.predict(test[['review_length', 'mnr', 'Maximum Content Similarity']])\n",
        "\n",
        "  # Assign the predicted labels to a new column 'fakeornot' in the test set\n",
        "  test['fakeornot'] = y_pred\n",
        "\n",
        "  # Obtain probability estimates for each class\n",
        "  probabilities = logreg.predict_proba(test[['review_length', 'mnr', 'Maximum Content Similarity']])\n",
        "\n",
        "  # Extract the probability of the positive class (class 1)\n",
        "  confidence_level = probabilities[:, 1]\n",
        "\n",
        "  # Add the confidence level to a new column 'confidence_level' in the test set\n",
        "  test['confidence level'] = confidence_level\n",
        "\n",
        "  fake = test.fakeornot.str.count(\"fake\").sum()\n",
        "  original = test.fakeornot.str.count(\"original\").sum()\n",
        "  fake_review = test['comment'].loc[(test.fakeornot == 'fake')]\n",
        "\n",
        "  return (fake,original, fake_review)\n",
        "\n",
        "\n",
        "\n",
        "pretrain('false_data(1).csv')\n"
      ]
    }
  ]
}